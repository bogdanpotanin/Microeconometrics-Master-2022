# --------
# Потанин Богдан Станиславович
# Микроэконометрика в R :)
# Семинар 2. Спецификация моделей бинарного выбора
# --------

library("switchSelection")                            # модель с гетероскдестичной 
                                                      # случайной ошибкой
                                                      # Оценим пробит модель без
                                                      # учета гетероскедастичности
library("lmtest")                                     # тестирование гипотез
library("numDeriv")                                   # библиотека для
                                                      # численного дифференцирования
library("margins")
library("hpa")                                        # полу-непараметрический подход
library("ggplot2")                                    # красивые графики
library("Ecdat")                                      # встроенные данные

# Отключим scientific notation
options(scipen = 999)

# Симулируем данные, содержащую информацию
# о характеристиках заемщика, а также о том,
# наступил ли у него дефолт по ипотеке.
# Все переменные измерены в условных единицах
set.seed(12345)                                            # для воспроизводимости
n     <- 10000                                             # число индивидов в выборке
h     <- data.frame(ind = rep(1:n))                        # датафрейм для хранения данных
h$inc <- runif(n, 0, 1)                                    # доход в условных единицах
h$pay <- runif(n, 0, 1)                                    # ежемесячный платеж
h$age <- runif(n, 0, 1)                                    # возраст
h$ins <- rbinom(n, 1, 0.7)                                 # факт наличия страховки
h$chl <- rbinom(n, 1, 0.6)                                 # факт наличия детей

# Оцениваемые коэффициенты
tau  <- c(0.8, -0.5)                                       # уравнение дисперсии
beta <- c(0.6, -3, 2, -5, 3.5, -0.8, 0, 0.8)               # уравнение среднего

# Генерация случайных ошибок
eps.var <- exp(tau[1] * h$pay +                            # дисперсия случайной ошибки
               tau[2] * h$chl) ^ 2                         # зависит от некоторых регрессоров
eps     <- rnorm(n, mean = 0, sd = sqrt(eps.var))          # случайная ошибка 

# Линейный предиктор
def_li   <- beta[1] +                                      # линейный индекс,
            beta[2] * h$inc +                              # отражающий вклад наблюдаемых
            beta[3] * h$pay +                              # факторов в вероятность дефолта
            beta[4] * h$age +
            beta[5] * h$age ^ 2 +                                            
            beta[6] * h$ins +                          
            beta[7] * h$chl +
            beta[8] * (h$chl * h$inc)

def_star <- def_li + eps                                   # латентная переменная,
                                                           # отражающая склонность
                                                           # к дефолту
h$def <- as.numeric(def_star >= 0)                         # наблюдаемое значение переменной
mean(h$def)                                                # доля дефолтов

h$ind <- NULL                                              # уберем индексы
head(h, 5)                                                 # посмотрим на данные

#---------------------------------------------------
# Часть 1. Учет гетероскедастичности в моделях
#          бинарного выбора
#---------------------------------------------------

# Кратко о предполагаемом процессе генерации данных
# в пробит модели с гетероскедастичной случайной
# ошибкой:
# y*_i = x_i * beta + e_i                основное уравнение
# e_i ~ N(0, sigma_i ^ 2)                случайная ошибка
# sigma_i = h(w_i * tau)                 уравнение дисперсии
# w_i * tau                              линейный индекс
#                                        уравнения дисперсии
# w_i                                    независимые переменные
#                                        влияющие на дисперсию
# h(0) = 1                               обычно накладываемые
# h'(0) != 0                             условия на функцию h()
# h(t) = exp(t)                          некоторые примеры
#        |t + 1|                         функции h(), удовтелвтяорющей
#        (t + 1) ^ 2                     данным условиям
# В данной модели в качестве оцениваемых параметров
# выступают векторы коэффициентов beta и tau

# Внесем некоторые корректировки в процесс 
# генерации данных

# Симулируем гетероскедастичные случайные ошибки, 
# предполагая, что их дисперсия может зависеть от
# платежей и факта наличия детей
set.seed(123)

                                                          # Симулируем зависимую переменную
def_star <- def_li + eps                                  # латентная переменная,
                                                          # отражающая склонность
                                                          # к дефолту
h$def    <- as.numeric(def_star >= 0)                     # наблюдаемое значение переменной
mean(h$def)                                               # доля дефолтов

## Оценим пробит модель без учеа гетероскедастичности
model.probit <- glm(formula = def ~ inc + pay +
                                    age + I(age ^ 2) +
                                    ins + chl +
                                    chl * inc,
                    data = h,                                  
                    family = binomial(link = "probit")) 
summary(model.probit)

# Оценим пробит модель с учетом гетероскедастичности
model.hetprobit <- msel(formula = def ~ inc + pay +            # линейный индекс
                                        age +                  # основного уравнения
                                        I(age ^ 2) +
                                        ins + chl +
                                        I(chl * inc) |         # линейный индекс
                                        pay + chl,             # уравнения дисперсии
                        data = h)
summary(model.hetprobit)

# Достанем полученные оценки
beta.est <- coef(model.hetprobit, type = "coef", eq = 1)       # оценки коэффициентов при переменных
                                                               # основного уравнения
tau.est  <- coef(model.hetprobit, type = "coef_var", eq = 1)   # оценки коэффициентов при переменных
                                                               # в уравнении дисперсии

# Включим константу в число регрессоров
beta.est <- c(-coef(model.hetprobit, type = "cuts", eq = 1), 
              beta.est)

# Сравним истинные значения и оценки коэффициентов линейного индекса
# уравнения дисперсии
cbind(true = tau, est = tau.est)

# Достанем оценки стандартных отклонений
# случайных ошибок
sigma.est <- predict(model.hetprobit, type = "sd")
head(sigma.est, 5)

# Осуществим тест на гомоскедастичность:
# H0: tau = 0 (гомосдкедастичность)
lrtest(model.hetprobit, model.probit)

# Спрогнозируем вероятности
prob.est <- predict(model.hetprobit, type = "prob", group = 1)
head(prob.est, 5)

# Рассчитаем предельные эффекты для индивида
Boris <- data.frame(inc = 0.2, pay = 0.1, age = 0.3, 
                    ins = 1,   chl = 1)

# Оценим предельный эффект возраста
me.age <- predict(model.hetprobit, newdata = Boris, type = "prob", 
                  group = 1,
                  me = "age",   # по какой переменной рассчитывается предельный эффект
                  test = TRUE)  # тестирование значимости предельного эффекта
print(me.age$tbl$val)           # величина предельного эффекта
summary(me.age)                 # результаты теста о равенстве предельного эффекта нулю

# Оценим предельный эффект наличия детей
me.chl <- predict(model.hetprobit, newdata = Boris, group = 1,
                  type = "prob", me = "chl", test = TRUE, eps = c(0, 1))
print(me.chl$tbl$val)
summary(me.chl)

# Подробная информация о расчетах предельных эффектов
help(predict.msel)

# ЗАДАНИЯ (* - средне, ** - сложно, *** - очень сложно)
# 1.1. Используя встроенные данные Mroz87 из библиотеки
#      sampleSelection и пробит модель с гетероскедастичной
#      случайно ошибкой определите, как на вероятность
#      занятости (lfp) влияют возраст (age), образование (educ),
#      факт проживания в городе (city) и число несовершеннолетних
#      детей (kids5 и kids618). При этом предполагается, что
#      гетероскедастичность может быть обусловлена возрастом
#      и уровнем образования. Далее, для 28-летнего индивида 
#      без высшего образования и с доходом 20000 оцените:
#      1)    вероятность занятости
#      2)    предельный эффект возраста на вероятность занятости
#      3)    предельный эффект проживания в городе на вероятность занятости
#      4*)   предельный эффект возраста на вероятность занятости, если
#            возраст входит в линейный индекс квадратично
#      5)    повторите предыдущие пункты, используя различные подходы
#            к определению формы уравнения дисперсии: см. аргумент link.scale
#      6**)  стандартную ошибку оценки вероятности занятости

#---------------------------------------------------
# Часть 2. Проверка гипотезы о гомоскедастичности
#          при помощи LM теста
#---------------------------------------------------

# Проверим гипотезу о гомоскедастичности
# при помощи LM теста, преимущество которого
# заключается в том, что нет необходимости
# предполагать конкретную форму функции h(t):
# достаточно наложить на нее лишь пару ограничений:
# 1) h(0) = 1                              
# 2) h'(0) != 0                             
HetprobitLnL <- function(x,                                # коэффициенты
                         y,                                # зависима переменная
                         X,                                # регрессоры основного уравнения
                         W,                                # регрессоры уравнения дисперсии
                         scale_fn = exp,                   # функция уравнения дисперсии h()
                         is_aggregate = TRUE)              # возвращаем функцию правдоподобия (TRUE)   
                                                           # или отдельные вклады (FALSE)          
{
  m_X <- ncol(X)
  m_W <- ncol(W)
  
  beta <- matrix(x[1:m_X], ncol = 1)                       # вектор beta коэффициентов и
  tau  <- matrix(x[(m_X + 1):(m_X + m_W)], ncol = 1)       # вектор дополнительных параметров  
                                                           # переводим в матрицу с одним столбцом
  
  y_li_mean     <- X %*% beta                              # оценка линейного индекса
  y_li_scale    <- W %*% tau                               # латентной переменной
  y_li_scale_fn <- scale_fn(y_li_scale)
  
  n_obs <- nrow(X)                                         # количество наблюдений
  
  L_vec <- matrix(NA, nrow = n_obs,                        # вектор столбец вкладов наблюдений
                  ncol = 1)                                # в функцию правдоподобия
  
  is_y_0 <- (y == 0)                                       # вектор условий y = 0
  is_y_1 <- (y == 1)                                       # вектор условий y = 1
  
  L_vec[is_y_1] <- pnorm(y_li_mean[is_y_1], 
                         sd = y_li_scale_fn[is_y_1])       # вклад наблюдений для которых yi = 1
  L_vec[is_y_0] <- 1 - pnorm(y_li_mean[is_y_0],
                             sd = y_li_scale_fn[is_y_0])   # вклад наблюдений для которых yi = 0
  
  lnL_vec <- log(L_vec)                                    # логарифмы вкладов
  
  if(!is_aggregate)                                        # возвращаем вклады
  {                                                        # при необходимости
    return(lnL_vec)
  }
  
  lnL <- sum(lnL_vec)                                      # логарифм функции правдоподобия
  
  return(lnL)
}

# Достанем данные
X.mat        <- cbind("(Intercept)" = 1,                   # регрессоры уравнения
                      model.hetprobit$W_mean[[1]])         # среднего
head(X.mat , 5)
W.mat <- model.hetprobit$W_var[[1]]                        # регрессоры уравнения
                                                           # дисперсии
head(W.mat , 5)
y.vec <- model.hetprobit$z                                 # зависимая переменная

# Достанем оценки ограниченной модели
x.est.R <- c(model.probit$coefficients, 
             rep(0, ncol(W.mat)))
n.R     <- length(x.est.R)                                 # добавим имена
names(x.est.R)[(n.R - 1):n.R] <- paste(colnames(W.mat),    # для красоты
                                       "var")
print(x.est.R)

# Рассчитаем правдоподобие полной модели в точке,
# определяемой оценками, полученными по ограниченной
# модели
lnL.R      <- HetprobitLnL(x.est.R, 
                           y = y.vec,
                           X.mat, W.mat, exp)
lnL.R.grad <- grad(func = HetprobitLnL,                    # считаем градиент данной функции
                   x = x.est.R,                            # численным методом
                   y = y.vec, 
                   X = X.mat, W = W.mat,
                   scale_fn = exp)                         # замените exp на function(x)
                                                           #                 {
                                                           #                   return(abs(x + 1)})
                                                           #                 }
                                                           # и убедитесь, что результат не изменится
lnL.R.grad <- matrix(lnL.R.grad, ncol = 1)                 # градиент как матрица с одним столбцом
lnL.R.Jac  <- jacobian(func = HetprobitLnL,                # оцениваем асимптотическую ковариационную
                       x = x.est.R,                        # матрицу при помощи Якобиана, расcчитанного
                       y = y.vec,                          # численным методом, поскольку численно
                       X = X.mat, W = W.mat,               # рассчитать Гессиан достаточно точным 
                       scale_fn     = exp,                 # образом не получается
                       is_aggregate = FALSE,
                       method.args  = list(r = 8))

# Реализуем тест
LM.value <- t(lnL.R.grad) %*%                              # считаем статистику теста
  qr.solve(t(lnL.R.Jac) %*% lnL.R.Jac) %*%                 # множителей Лагранжа
  lnL.R.grad
p.value <- 1 - pchisq(LM.value, df = ncol(W.mat))          # рассчитываем p-value теста

# Аналитический подход
li.R       <- X.mat %*% x.est.R[1:ncol(X.mat)]  
jac.gamma  <- (dnorm(li.R) / 
               pnorm((2 * y.vec - 1) * li.R)) * 
              (1 - 2 * y.vec) * li.R
jac.gamma  <- as.vector(jac.gamma) * W.mat
jac.beta   <- (dnorm(li.R) / pnorm((2 * y.vec - 1) * 
               li.R)) * (2 * y.vec - 1)
jac.beta   <- as.vector(jac.beta) * X.mat
jac        <- cbind(jac.beta, jac.gamma)
head(jac, 3)
gr         <- colSums(jac)
LM.value.2 <- gr %*% solve(t(jac) %*% jac) %*% gr
p.value.2  <- 1 - pchisq(LM.value.2, df = ncol(W.mat))

#---------------------------------------------------
# Часть 3. Тестирование гипотезы о нормальном
#          распределении случайных ошибок
#---------------------------------------------------

# Оценим пробит модель
model.probit <- glm(formula = def ~ inc + pay +            # оцениваем модель
                                    age + I(age ^ 2) +
                                    ins + chl +
                                    I(chl * inc),
                    data    = h,                                  
                    family  = binomial(link = "probit")) 

# Запишем функцию правдоподобия
# для модели со случайно ошибкой
# из распределения Пирсона
ProbitLnLExtended <- function(par,                         # вектор значений параметров
                              y,                           # зависимая переменная 
                              X,                           # матрица независимых переменных
                              is_aggregate = TRUE)         # при TRUE возвращаем логарифм
                                                           # функции правдоподобия, а при
                                                           # FALSE возвращаем вектор вкладов
{
  beta  <- matrix(par[-c(1, 2)], ncol = 1)                 # вектор beta коэффициентов и
  theta <- matrix(par[c(1, 2)], ncol = 1)                  # вектор дополнительных параметров  
                                                           # переводим в матрицу с одним столбцом
  y_li  <- X %*% beta                                      # оценка линейного индекса
  y_est <- y_li + theta[1] * y_li ^ 2 +                    # оценка математического ожидания 
           theta[2] * y_li ^ 3                             # латентной переменной
  
  n_obs <- nrow(X)                                         # количество наблюдений
  
  L_vec <- matrix(NA, nrow = n_obs,                        # вектор столбец вкладов наблюдений
                  ncol = 1)                                # в функцию правдоподобия
  
  is_y_0 <- (y == 0)                                       # вектор условий (y = 0)
  is_y_1 <- (y == 1)                                       # вектор условий (y = 1)
  
  L_vec[is_y_1] <- pnorm(y_est[is_y_1])                    # вклад наблюдений для которых yi = 1
  L_vec[is_y_0] <- 1 - pnorm(y_est[is_y_0])                # вклад наблюдений для которых yi = 0
  
  lnL_vec <- log(L_vec)                                    # логарифмы вкладов
  
  if(!is_aggregate)                                        # возвращаем вклады
  {                                                        # при необходимости
    return(lnL_vec)
  }
  
  lnL <- sum(lnL_vec)                                      # логарифм функции правдоподобия
  
  return(lnL)
}

# Воспользуемся созданной функцией
# Оценки модели при справедливом ограничении,
# накладываемом нулевой гипотезой
beta.est               <- coef(model.probit)               # достаем оценки из обычной пробит
beta.R                 <- c(0, 0, beta.est)                # модели и приравниваем значения
names(beta.R)[c(1, 2)] <- c("theta1", "theta2")            # дополнительных параметров к значениям,
                                                           # предполагаемым нулевой гипотезой
print(beta.R)

# Создадим матрицу регрессоров
X.mat      <- as.matrix(model.frame(model.probit))         # достаем датафрейм с регрессорами и
X.mat[, 1] <- 1                                            # первращаем его в матрицу, а также
colnames(X.mat)[1] <- "Intercept"                          # заменяем зависимую переменную на константу
head(X.mat, 5)

# Применим функцию
lnL.R <- ProbitLnLExtended(beta.R, h$def, X.mat)           # считаем логарифм функции правоподобия
                                                           # при ограничениях, совпадающую с логарифмом
                                                           # функции правдоподобия обычной пробит модели
lnL.R.grad <- grad(func = ProbitLnLExtended,               # считаем градиент данной функции
                   x    = beta.R,                          # численным методом
                   y    = h$def, 
                   X    = X.mat)
lnL.R.grad <- matrix(lnL.R.grad, ncol = 1)                 # градиент как матрица с одним столбцом
lnL.R.Jac <- jacobian(func         = ProbitLnLExtended,    # считаем Якобин данной функции
                      x            = beta.R,               # численным методом
                      y            = h$def, 
                      X            = X.mat,
                      is_aggregate = FALSE)

# Реализуем тест
LM.value.1 <- t(lnL.R.grad) %*%                            # считаем статистику теста
              qr.solve(t(lnL.R.Jac) %*% lnL.R.Jac) %*%     # множителей Лагранжа
              lnL.R.grad
p.value_1  <- 1 - pchisq(LM.value.1, df = 2)               # рассчитываем p-value теста
                                                           # множителей Лагранжа

# Повторим тест с использованием регрессии на единицы

# Достанем датафрейм, содержащий переменные модели
d <- model.frame(model.probit)                             # все переменные

# Рассчитаем предварительные величины
y.li.est <- predict(model.probit)                          # оценка линейного индекса                                     
F.est    <- pnorm(y.li.est)                                # функции от линейного           
f.est    <- dnorm(y.li.est)                                # индекса

# Вычислим обобщенные остатки
gr <- ((d[, 1] - F.est) /                                  # обобщенный остаток
         (F.est * (1 - F.est))) * f.est

# Считаем производные по коэффициентам
d_beta <- apply(X.mat, 2, function(x)                      # производные по
{                                                          # регресcионным коэффициентам
  x * gr
})
d_t1 <- (gr * y.li.est ^ 2)                                # производная по t1
d_t2 <- (gr * y.li.est ^ 3)                                # производная по t2

# Сравним аналитические и численные производные
grad_df <- data.frame("Numeric"    = lnL.R.grad,
                      "Analytical" = colSums(cbind(d_t1, 
                                                   d_t2, 
                                                   d_beta)))
rownames(grad_df) <- c("t1", "t2", colnames(X.mat))
print(grad_df)

# Проводим LM тест
n     <- nrow(d)                                           # число наблюдений
LM_df <- data.frame("my_ones" = rep(1, n),                 # вектор из единиц 
                    "d_"      = d_beta,
                    d_t1, d_t2)          
head(LM_df, 5)
ones_regression <- summary(lm(my_ones ~. + 0,              # регрессия на вектор единиц без константы
                              data = LM_df))       
R2         <- ones_regression$r.squared                    # коэффициент детерминации регрессии
LM_value_2 <- R2 * n                                       # LM статистика
p.value_2  <- 1 - pchisq(q  = LM_value_2, 
                         df = 2)

# Сравним полученные результаты и убедимся,
# что они полностью совпадают
c(LM.value.1, LM_value_2)                                  # сравниваем статистики
c(p.value_1, p.value_2)                                    # сравниваем p-value

#---------------------------------------------------
# Часть 4. Метод Галланта и Нички
#---------------------------------------------------

# По мотивам статьи:
# Parametric and Semi-Parametric Estimation of the 
# Binary Response Model of Labour Market

# Рассмотрим пример на данных, отражающих 
# занятость индивидов
help(Participation)

# Загрузим данные
data("Participation")
h         <- Participation
h$lfp     <- as.numeric(h$lfp     == "yes")
h$foreign <- as.numeric(h$foreign == "yes")
h$age     <- h$age * 10

# Пробит модель, описывающая занятость
model_pr <- glm(formula = lfp ~ lnnlinc +                  # логарифм нетрудового дохода
                                age + I(age ^ 2) +         # возраст   
                                educ +                     # образование в годах
                                nyc +                      # к-во маленьких детей
                                noc +                      # к-во взрослых детей
                                foreign,                   # иностранец 
                data    = h,                                     
                family  = binomial(link = "probit"))  
summary(model_pr)
coef_pr <- coef(model_pr)

# Как и в оригинальной работе будем использовать
# полином третьей степени
model_hpa <- hpaBinary(formula   = lfp ~ I(-lnnlinc) +     # нормализуем коэффициент при 
                                         age +              # регрессоре lnnlinc к -1
                                         I(age ^ 2) +  
                                         educ +                      
                                         nyc +                       
                                         noc +                       
                                         foreign,                   
                       data     = h, 
                       K        = 3,                       # степень полинома
                       cov_type = "sandwich")              # тип ковариационной матрицы
summary(model_hpa)
coef_hpa <- model_hpa$coefficients                         # достанем оценки коэффициентов

# Альтернативный сопособ оценивания без
# фиксирования коэффициентов
model_hpa2 <- msel(formula = lfp ~ lnnlinc + age +
                                   I(age ^ 2) + educ + 
                                   nyc + noc + foreign,                   
                   data = h,marginal = list(hpa = 3))
coef_hpa2 <- coef(model_hpa2, type = "coef", eq = 1)       # достанем оценки коэффициентов
summary(model_hpa2)

# Визуализируем результат
plot(model_hpa)                                            # оценка функции плотности
                                                           # случайной ошибки
# Сравним модели по AIC
AIC(model_hpa)    
AIC(model_hpa2)   
AIC(model_pr)

# Сравним оценки коэффициентов
data.frame("Galland.Nychka" = coef_hpa2, 
           "Probit" = coef_pr[-1])

# Рассмотрим пример на данных, отражающих готовность
# людей платить за сохранение парка
help(Kakadu)

# Загрузим данные
data("Kakadu")
h     <- Kakadu
h$wtp <- as.numeric(h$answer != "nn")                      # переменная принимает значение 1,
                                                           # если индивид готов заплатить за
                                                           # сохранение парка больше некоторой суммы
h$sex <- as.numeric(h$sex)

# Модель, описывающая готовность индивида
# заплатить более некоторой суммы
model_pr <- glm(formula = wtp ~ mineparks +                # открытие производства в парковых
                                                           # зонах существенно уменьшает
                                                           # привлекательность парка
                                age +                      # возраст
                                sex +                      # пол (мужчина)
                                income +                   # доход в тысячах долларов
                                moreparks +                # нужно больше парков
                                wildlife +                 # важно сохранять дикую природу
                                aboriginal +               # важно учитывать интересы 
                                                           # коренных жителей
                                finben,                    # важно руководствоваться соображениями
                                                           # финансовой выгоды при использовании
                                                           # природных ресурсов
                data   = h,                                    
                family = binomial(link = "probit"))          
summary(model_pr)

# Применим полунепараметрическую модель
model_hpa <- hpaBinary(formula  = formula(model_pr),                   
                       data     = h, 
                       K        = 6,                       # степень полинома
                       cov_type = "sandwich")              # тип ковариационной матрицы
summary(model_hpa)

# Альтернативный способ
model_hpa2 <- msel(formula = formula(model_pr),                   
                   data = h, marginal = list(hpa = 6))
summary(model_hpa2)

# ВАЖНО:
# В зависимости от типа ковариационной матрицы
# можно получить различный подход к интерпретации
# оценок:
# cov_type = "sandwich"  - квази-максимальное правдоподобие
# cov_type = "bootstrap" - полу-непараметрический подход
# cov_type = "hessian"   - параметрический подход


# Визуализируем результат
plot(model_hpa)

# Сравним модели по AIC
AIC(model_pr)
AIC(model_hpa)
AIC(model_hpa2)

# Сравним предсказанные вероятности
p_pr   <- predict(model_pr, type = "response")
p_hpa  <- predict(model_hpa, is_prob = TRUE)
p_hpa2 <- predict(model_hpa2, type = "prob", group = 1)
head(cbind(probit = p_pr, GN = p_hpa, GN2 = h_hpa2))
plot(p_pr, p_hpa,                                          # визуально сравнение оценок
     xlab = "Gallant and Nychka", ylab = "Probit")         # вероятностей

# Сравним предельные эффекты пробит модели
# и полученные при помощи метода Галланта и Нички
ME_hpa    <- model_hpa$marginal_effects                    # Галланта и Ничка
ME_probit <- margins(model_pr, type = "response")          # Пробит модель
plot(ME_hpa[, "age"], ME_probit$dydx_age,                  # визуально сравнение оценок
     xlab = "Gallant and Nychka", ylab = "Probit")         # предельного эффекта возраста

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 4.1.    Подберите оптимальную степень полинома K
#         руководствуясь соображениями минимизации
#         информационного критерия Акаике
# 4.2.    Сравните оценки асимптотической ковариационной
#         матрицы, получаемые различными методами
# 4.3.    Используя функцию dhpa аналитически оцените:
#         1*)   вероятность того, что индивид с вашими
#               характеристиками будет готов платить
#               за сохранение парка
#         2**)  предельный эффект возраста на
#               соответствующую вероятность